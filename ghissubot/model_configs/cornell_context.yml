model: AttentionSeq2SeqWithContext
model_params:
  attention.class: seq2seq.decoders.attention.AttentionLayerDot
  attention.params:
    num_units: 256

  bridge.class: seq2seq.models.bridges.ZeroBridge

  embedding.dim: 128
  embedding.share: true

  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder
  encoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params:
        num_units: 256
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1

  context_encoder.class: ghissubot.encoders.ConvContextEncoder
  context_encoder.params:
      frozen_graph_dir: trained_models/context_encoder_frozen
      frozen_graph_filename: context_cnn_frozen.pb
      max_sequence_len: 25
      naming_prefix: ghissubot_subgraph/inference_ops
      input_tensor_name: cnn_input_utterances
      output_tensor_name: hidden_context_layer/Relu
      dropout_prob_name: dropout_keep_prob
      table_init_op_name: ghissubot_subgraph/init_all_tables

  decoder.class: seq2seq.decoders.AttentionDecoder
  decoder.params:
    rnn_cell:
      cell_class: LSTMCell
      cell_params:
        num_units: 256
      dropout_input_keep_prob: 0.8
      dropout_output_keep_prob: 1.0
      num_layers: 1

  optimizer.name: Adam
  optimizer.params:
    epsilon: 0.0000008
  optimizer.learning_rate: 0.0005

  source.max_seq_len: 50
  source.reverse: false
  target.max_seq_len: 50

  vocab_source: data/cornell_film/context/cornell_vocab_31000.txt
  vocab_target: data/cornell_film/context/cornell_vocab_31000.txt

input_pipeline_train:
  class: ConversationInputPipeline
  params:
    source_files:
        - data/cornell_film/context/utterance_train.txt
    target_files:
        - data/cornell_film/context/response_train.txt
    num_epochs: 19

input_pipeline_dev:
  class: ConversationInputPipeline
  params:
    source_files:
        - data/cornell_film/context/utterance_dev.txt
    target_files:
        - data/cornell_film/context/response_dev.txt
